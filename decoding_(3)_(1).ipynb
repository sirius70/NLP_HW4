{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirius70/NLP_HW4/blob/main/decoding_(3)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu66L-I5tsXo"
      },
      "source": [
        "# Language Model Decoding Algorithms\n",
        "\n",
        "In this notebook, you will implement several language model decoding algorithms, from simple greedy decoding to beam search. You will test these algorithms using GPT-2 as implemented in the huggingface `transformers` library. You will evaluate the results of an open-ended story generation task, where the goal is not to match a particular human-written reference text and more to generate text that is fluent and diverse. This gives more scope to different decoding approaches.\n",
        "\n",
        "The `transformers` library implements most of these decoding algorithms, but you will be implementing them yourself using the capabilities of the underlying pytorch library.\n",
        "\n",
        "This code will benefit from running on a GPU.  If you're running on Colab, **choose \"Runtime Type\" = GPU for running this notebook (Runtime > Change Runtime Type > T4 GPU).**\n",
        "\n",
        "The evaluation and harness code for beam search is based on an earlier assignment by Jaehun Jung, Gary Jiacheng, and Yejin Choi from the University of Washington.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Please make sure to run all cells in the setup section, even though you won't implement anything yet. First, we install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lSg7SGaR5fM",
        "outputId": "5b956415-f21a-4384-cd3b-51718fdd6258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.12/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (2025.10.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.38.2\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2HajK5vaM9N"
      },
      "source": [
        "Now we check available devices and set the randomizer seed for replicability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5S6UO84y3fj",
        "outputId": "de35e5ef-2d22-48a8-89fb-faea8b7ceb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S8uX6e1uCEU"
      },
      "source": [
        "We load a dataset of prompts for open-ended story generation derived from the [ROCStories corpora](https://cs.rochester.edu/nlp/rocstories/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPcsD3_Gqm7P",
        "outputId": "641a3f55-14fa-4c84-844a-1cee531999dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'story_id': '080198fc-d0e7-42b3-8e63-b2144e59d816', 'prompt': 'On my way to work I stopped to get some coffee.', 'continuation': 'I went through the drive through and placed my order. I paid the cashier and patiently waited for my drink. When she handed me the drink, the lid came off and spilled on me. The coffee hurt and I had to go home and change clothes.', 'constraint_words': ['drive', 'order', 'drink', 'lid', 'coffee', 'hurt', 'home', 'change', 'clothes']}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_dqyjUBuQlQ"
      },
      "source": [
        "Unlike more constrained generation tasks such as translation and summarization, we cannot usefully check the generated output against a single human reference. Instead, we evaluate its overall fluency, diversity, and similarity to other English texts.\n",
        "\n",
        "To score the English **fluency** of generated sentences, we will use the CoLA classifier.  This is a RoBERTa-large classifier trained on the CoLA corpus (Warstadt et al., 2019), which contains sentences paired with grammatical acceptability judgments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "d8d00ab325954ddfb7f86545d7226d4e",
            "d1461b82a1aa4f91b3ad96a53b662ded",
            "0e0d929e59184b06a732f8a8827fa5aa",
            "d6329fb3004243c4949f74e038458cbd",
            "7f5df3df43ea48e392a5e3e5e69be7c2",
            "02cc83919e67436ab5ab6d590850d202",
            "f8c23664b12d4f7dae1f7d8e149bce58",
            "12b3552d0efd4703bf095a623c750e87",
            "207c80e818a4436f802ccb556a46c7ed",
            "2fb08f98ea6d45d2b98ea800af38e9e4",
            "93fcf725d71f46719dcb157dcc009a71"
          ]
        },
        "id": "gmMd57VjSEZY",
        "outputId": "513ad41d-7709-41d8-c1ad-e0377102f481"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8d00ab325954ddfb7f86545d7226d4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-m8_EsPGFQU"
      },
      "source": [
        "To evaluate **diversity**, we will simply count the number of distinct 1-, 2-, and 3-grams in the output.\n",
        "\n",
        "To evaluate **naturalness**, or similarity to other English texts, we will use the perplexity of the output under the model, i.e., GPT-2. Think about which decoding method you would expect to perform the best on this metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "9dad151287214623ada40bd841eca160",
            "d6d956caa1134986a95d508ceee80fbd",
            "b512fd2388a141d7a6d07367f90249d2",
            "aa1945e9931a4ac2b345fc90f9744576",
            "4d852c966d534dd2a7759783097955fa",
            "70bd0ff4c25749c9b496152ac19719b3",
            "c6135cbcf793428fb36baae9bbbf3d15",
            "4b48084f67d14d4f89a149c10457d447",
            "afa91ad3f4a247b2af4bb96611dabbd7",
            "be7e852cee8543cf91c0eae329af61d6",
            "cf83f5b8957d4f319ecc394bb864a975"
          ]
        },
        "id": "7t97XqNh2Xu2",
        "outputId": "9256b453-c4ea-4f21-b6f5-b533a2edd59e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dad151287214623ada40bd841eca160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "debugging run\n",
            "perplexity = 178.64\n",
            "fluency = 0.98\n",
            "diversity = 0.87, 1.00, 1.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  fluency = compute_fluency(generations)\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(experiment)\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()\n",
        "\n",
        "debug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "evaluate(debug_sents, 'debugging run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGaWrCTzzIqA"
      },
      "source": [
        "We install GPT-2 and its associated tokenizer from huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5_8Bym3HAq",
        "outputId": "d03dcb04-5617-481f-96cf-d5b54ee5cb77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqgPTfEYHJso"
      },
      "source": [
        "## Batching and Working with Tensors\n",
        "\n",
        "To improve the throughput of language model training and inference, especially on GPUs, we usually group together inputs in **batches**.  What this means for your generation code is that at each function call, you will decide on the next token for several inputs in parallel. Instead of a single vector of the pre-softmax logits of the next token, you will get a two-dimensional tensor of shape $B \\times V$ for batch size $B$ and token vocabulary size $V$.\n",
        "\n",
        "Consider a toy example of a random $4 \\times 5$ matrix $A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgsquTH9na9S"
      },
      "outputs": [],
      "source": [
        "A = torch.randn(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26VCjPMSnrDM"
      },
      "source": [
        "**TODO**: Familiarize yourself with some of the functions on tensors that will be useful for implementing the different decoding algorithms below: `torch.argmax`, `torch.multinomial`, `torch.nn.functional.softmax`, `torch.squeeze`, `torch.topk` and others. You can consult [pytorch's documentation](https://docs.pytorch.org/docs/stable/index.html) or other resources. These functions and others in the pytorch library are all fine to use in your code below. This cell is not graded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlU641QspQxE",
        "outputId": "e6f6a777-2f9d-4859-c5f0-ea4a8ed35b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.argmax - Find indices of maximum values\n",
            "Max index along dim=1 (row-wise): tensor([0, 1, 2, 3])\n",
            "Max index along dim=0 (column-wise): tensor([3, 1, 2, 3, 3])\n",
            "Max index in entire tensor: tensor(6)\n",
            "F.softmax - Convert to probabilities\n",
            "Probabilities (each row sums to 1):\n",
            "tensor([[0.4052, 0.1135, 0.1859, 0.1457, 0.1497],\n",
            "        [0.1286, 0.4678, 0.2296, 0.1559, 0.0180],\n",
            "        [0.1142, 0.1958, 0.4274, 0.1892, 0.0733],\n",
            "        [0.2637, 0.1358, 0.0905, 0.3233, 0.1868]])\n",
            "Row sums (should all be 1.0): tensor([1.0000, 1.0000, 1.0000, 1.0000])\n",
            "torch.multinomial - Sample from distribution\n",
            "Sampled indices (one per row): tensor([0, 1, 2, 1])\n",
            "Multiple samples (3 per row):\n",
            "tensor([[0, 1, 0],\n",
            "        [2, 1, 1],\n",
            "        [0, 0, 2],\n",
            "        [0, 4, 1]])\n",
            "\n",
            "Cumulative probabilities for top-p:\n",
            "Sorted probs: tensor([0.4052, 0.1859, 0.1497, 0.1457, 0.1135])\n",
            "Cumulative probs: tensor([0.4052, 0.5911, 0.7409, 0.8865, 1.0000])\n",
            "Tokens needed for 0.9 probability mass: tensor(5)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Familiarize yourself with some pytorch functions by manipulating the matrix A.\n",
        "print(\"torch.argmax - Find indices of maximum values\")\n",
        "print(\"Max index along dim=1 (row-wise):\", torch.argmax(A, dim=1))\n",
        "print(\"Max index along dim=0 (column-wise):\", torch.argmax(A, dim=0))\n",
        "print(\"Max index in entire tensor:\", torch.argmax(A))\n",
        "\n",
        "\n",
        "# torch.softmax - convert to probability distribution\n",
        "print(\"F.softmax - Convert to probabilities\")\n",
        "probs = F.softmax(A, dim=-1)  # dim=-1 means last dimension (row-wise)\n",
        "print(\"Probabilities (each row sums to 1):\")\n",
        "print(probs)\n",
        "print(\"Row sums (should all be 1.0):\", probs.sum(dim=1))\n",
        "\n",
        "\n",
        "# torch.multinomial - sample from probability distribution\n",
        "print(\"torch.multinomial - Sample from distribution\")\n",
        "samples = torch.multinomial(probs, num_samples=1)\n",
        "print(\"Sampled indices (one per row):\", samples.squeeze())\n",
        "samples_multiple = torch.multinomial(probs, num_samples=3, replacement=True)\n",
        "print(\"Multiple samples (3 per row):\")\n",
        "print(samples_multiple)\n",
        "\n",
        "\n",
        "print(\"\\nCumulative probabilities for top-p:\")\n",
        "sorted_probs, sorted_indices = torch.sort(probs[0], descending=True)\n",
        "cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "print(\"Sorted probs:\", sorted_probs)\n",
        "print(\"Cumulative probs:\", cum_probs)\n",
        "print(\"Tokens needed for 0.9 probability mass:\", (cum_probs <= 0.9).sum() + 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfwAdXU4urVp"
      },
      "source": [
        "## Basic Decoding Algorithms\n",
        "\n",
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Simple sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSHpk5vKyQ4U"
      },
      "outputs": [],
      "source": [
        "def decode(prompts, max_len, method, **kwargs):\n",
        "  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  input_ids = encodings_dict['input_ids'].to(device)\n",
        "  attention_mask = encodings_dict['attention_mask'].to(device)\n",
        "\n",
        "  model_kwargs = {\n",
        "      'attention_mask': attention_mask,\n",
        "      'cache_position': torch.arange(input_ids.shape[1], device=device)\n",
        "  }\n",
        "  batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "  for step in range(max_len):\n",
        "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "    if step == 0:\n",
        "      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n",
        "    else:\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    if method == 'greedy':\n",
        "      next_tokens = greedy(next_token_logits)\n",
        "    elif method == 'sample':\n",
        "      next_tokens = sample(next_token_logits)\n",
        "    elif method == 'temperature':\n",
        "      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n",
        "    elif method == 'topk':\n",
        "      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n",
        "    elif method == 'topp':\n",
        "      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n",
        "\n",
        "    # finished sentences should have their next token be a padding token\n",
        "    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n",
        "\n",
        "    # if eos_token was found in one sentence, set sentence to finished\n",
        "    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n",
        "\n",
        "    if unfinished_sequences.max() == 0:\n",
        "      break\n",
        "\n",
        "  response_ids = input_ids[:, input_seq_len:]\n",
        "  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n",
        "\n",
        "  return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-bghfZe30wN"
      },
      "outputs": [],
      "source": [
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0]['prompt']] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "  for prompt, generation in zip(prompts, generations):\n",
        "    print(f'{[prompt]} ==> {[generation]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryFGrlRSXYn"
      },
      "source": [
        "### Greedy Decoding\n",
        "\n",
        "The simplest strategy is to select the token with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH0wBhy2SZa-",
        "outputId": "bb14741f-f75a-4902-b2ef-f4ede2bd3525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement greedy decoding\n",
        "\n",
        "def greedy(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Compute `next_tokens` from `next_token_logits`.\n",
        "  next_tokens = torch.argmax(next_token_logits,dim = -1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "generations = decode(dev_prompts, max_len=20, method='greedy')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYghJYko1X13"
      },
      "source": [
        "**TODO**: What do you observe when the code above generates 10 times from the test prompt?\n",
        "\n",
        "Greedy decoding produces identical,repetitive outputs across all generations.Since it selects the highest probabaility token at each step,there is no variation between samples- all 10 generations are eaxctly the same,the model gets stuck in a loop repeating the same phrase pattern.This happens because greedy decoding always pick the most probabale next token,so once it gets into a pattern,it keeps repeating it deterministically with no variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKL_31VJNw1"
      },
      "source": [
        "### Simple Sampling and Temperature Sampling\n",
        "\n",
        "Greedy decoding has some drawbacks that we discussed in class. One alternative is simply to sample from the probability distribution of the language model. We'll call that _simple sampling_ to distinguish it from more complicated methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjrTLKj2JR5b",
        "outputId": "04f25e54-d13e-443c-8661-1a2df48c6a02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nKiddiak said he stepped on it, hit the brakes hard and decided to cross first']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" Their interaction didn't work at all by the time he would get back to Louisville. Some details that\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' According to investigators at the time, Gianfranco was yelling at them for not doing enough work']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"No, my boyfriend doesn\\'t do work,\" Cat started, desperate to dial a phone in distress']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Later, he did not show up for school again until late the next day. After catching a train']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The fliers denounced Gov. Pat Brown for having insufficient police presence in the building, and he engineered']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Then-FBI director Christopher Wray cut him off. The FBI is supposed to press charges against Matt']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The only choice was his motorcycle. His smile vanished. About 28 miles (50 km) away was']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Just before giving up his job, the first thing he learned was who withdrew the moneyâ€¦the grain']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I walked off at 3 in the morning. Hopefully your husband has a lot deeper wisdom and his']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement simple sampling.\n",
        "def sample(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Compute the probabilities from the logits,\n",
        "  # then compute `next_tokens` from `probs`.\n",
        "  # Hint: `probs` should have size (B, V)\n",
        "  probs = F.softmax(next_token_logits,dim = -1)\n",
        "\n",
        "  next_tokens = torch.multinomial(probs,num_samples =1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='sample')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU-jtevc1_yO"
      },
      "source": [
        "**TODO**: What do you observe when you generating 10 times from the test prompt?\n",
        "\n",
        "Simple Scaling produces diverse,varied outputs across all generations,unlike greedy decoding,samples tokens from the probability distribution,introducing randomness that prevents repetitive patterns and generates unique text each time.While diversity  increase,some outputs,may be less,coherent or drift off topic.Each of the 10 generations produces completely different text -no outputs are the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNFfm1kXp_4f"
      },
      "source": [
        "The probability distribution of the model has some undesirable properties, as we discussed in class. You can adjust its entropy using a _temperature_ parameter `t`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Su03uzJb_Z",
        "outputId": "490a97f2-12d1-4224-ff41-9cc8ed79e37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nKiddiak said he stepped on the blood spur out of pity from his cousin Jimmie']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" Their interaction didn't go well.\\n\\nHerad's mother was in a panic. Unable to\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He said he hadn't asked if he'd be coming back to work for a week.\\n\\n\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I told my boss I was leaving work early to take care of my sons\\' homework. So']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He also said he was going to play out of town because the game was cancelled. Johnson also said']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The next morning, after talking to other parents who also work in the area, he was told to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' It was before he was going to college.\\n\\n\"Oh, I\\'ll do it,\" he']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The only choice was to get out of school and go to her house to celebrate the graduation. After']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Just before noon, he was drumming his drums in front of his house. He saw his friend']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I walked off the job, I went back to school, I took a couple of hours off']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement temperature-scaled sampling.\n",
        "\n",
        "def temperature(next_token_logits, t):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - t: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "  scaled_logits = next_token_logits/t\n",
        "  probs = F.softmax(scaled_logits,dim = -1)\n",
        "  next_tokens = torch.multinomial(probs,num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYIAzmF2MD2"
      },
      "source": [
        "**TODO**: What value of `t` would be equivalent to greedy decoding? What value of `t` would be equivalent to simple sampling?\n",
        "\n",
        "Greedy decoding t -> 0 as it approaches zero.As temperature approaches 0,the softmax becomes more peaked,making the highest probability token approach probability 1.This effectively makes the samppling deterministic,where it always picks argmax.\n",
        "\n",
        "Simple sampling t = 1.0\n",
        "Temperature of 1 leaves the logits unchanged,and then giving us the original probability distribution from the model for sampling.\n",
        "\n",
        "Makes distribution flatter more uniform where it is more random or creative. when t is greater than 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZZorDeWRVtm"
      },
      "source": [
        "### Top-k Sampling\n",
        "\n",
        "By Zipf's law, the distribution over tokens contains many low-probability words in the tail. It might be useful to sample only from the most probable words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNHk9mjXRdNA",
        "outputId": "d6a4753d-0350-462a-bd8b-9839d089020c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I wasn\\'t sure what to expect, so he asked me why I didn\\'t work']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I\\'m tired of talking about this because it\\'s a big distraction. I\\'m tired']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' When he arrived at work, the other workers were on the line and asked if he would be willing']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"Why don\\'t you try to do work? You don\\'t look like a person,\" his friend']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He said he did not have enough money to pay his bills. He told his girlfriend, a teacher']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The next morning, after work, he would get back to his job, and the next day,']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" It was the first time they had ever met, and they couldn't quite decide how to respond:\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The only problem was that his roommate was working at a local hospital when she refused to come out because']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' A few days were spent at home in his home in Washington, D.C., a short drive']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I wanted a ride to New York with my family,\" said the 33-year-old New']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement top-k sampling\n",
        "\n",
        "def topk(next_token_logits, k):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - k: int\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Keep only top-k tokens with highest probabilities.\n",
        "  # Hint: Create a mask to zero out all logits not in top-k\n",
        "  top_k_logits,top_k_indices = torch.topk(next_token_logits,k,dim =-1)\n",
        "  mask = torch.full_like(next_token_logits,float('-inf'))\n",
        "  mask.scatter_(-1,top_k_indices,top_k_logits)\n",
        "\n",
        "  probs = F.softmax(mask,dim = -1)\n",
        "  next_tokens = torch.multinomial(probs,num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topk', k=20)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHFioE3V3WAD"
      },
      "source": [
        "**TODO**: What value of `k` makes this equivalent to simple sampling?\n",
        "\n",
        "When k equals the full vocabulary size,all tokens are kept in the top-k ,so no tokens were masked out .This makes top-k sampling equivalent to simple sampling since we are sampling from the complete probabaility distribution.When k equals the vocabulary size(V) ,the torch.topk()nfunction returns all tokens ,so the mask does not filter anything out,We ended from the complete original probabaility distribution which is exactly what simple sampling does.Small k as only top 20 most probable tokens are considered ,making the generation more focused and conservative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSjMWNFEy_cC"
      },
      "source": [
        "### Top-p Sampling\n",
        "\n",
        "But should we sample from the same number of candidates in all contexts? Maybe in different contexts the number of good candidates is larger or smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq1ZwVVxzApa",
        "outputId": "992d34de-ed34-438e-9604-8b49c3490917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [\"\\n\\nKiddie said he also didn't know if he was working out or not, and\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' On the day of the attack, he took the opportunity to share a beautiful moment with her, a']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' When he arrived at work, the new tenant was coming in with a pile of trash.\\n\\n']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"No, my boyfriend doesn\\'t do that,\" he said, before jogging out to his room']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He also sat in a wheelchair at his hospital bed. In the room was a young woman who looked']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" The rookie's parents said he was talking to his father and asking him to help him.\\n\\n\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" Then he goes back to work. He can't afford to do it. So he goes back to\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The only problem was that his roommate was working at a local Walmart when she refused to work, because']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' A low-level CIA agent from the Pentagon who was employed by Special Operations Command was present. But']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I walked up and there was a white man. I said, \\'Do you have any idea']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement top-p (nucleus) sampling.\n",
        "\n",
        "def topp(next_token_logits, p):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - p: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Sort the logits in descending order, and compute\n",
        "  # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "  # and then sample from those that make up the top p probability mass.\n",
        "  sorted_logits,sorted_indices = torch.sort(next_token_logits,descending=True)\n",
        "  sorted_probs = F.softmax(sorted_logits,dim = -1)\n",
        "  cum_probs = torch.cumsum(sorted_probs,dim = -1)\n",
        "\n",
        "  sorted_indices_to_remove = cum_probs > p\n",
        "  sorted_indices_to_remove[...,1:] = sorted_indices_to_remove[...,:-1].clone()\n",
        "  sorted_indices_to_remove[...,0] = False\n",
        "\n",
        "  sorted_logits[sorted_indices_to_remove] = float('-inf')\n",
        "\n",
        "  mask = torch.full_like(next_token_logits,float('-inf'))\n",
        "  mask.scatter_(-1,sorted_indices,sorted_logits)\n",
        "\n",
        "  probs = F.softmax(mask,dim = -1)\n",
        "\n",
        "  next_tokens = torch.multinomial(probs,num_samples = 1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB_lyjZr3i48"
      },
      "source": [
        "**TODO**: What value of `p` makes this equivalent to simple sampling?\n",
        "\n",
        "When p is 1.0,the cumulative proabability threshold includes all tokens,100 percent of the probability mass,so no tokens are filtered out.This makes top k sampling equivalent to simple sampling since we are sampling from the complete probability distribution.When p is 1.0 cumulative probability  it never exceeds 1.0 so cum_probs is greater than p is always False.So this means no tokens are removed and sample from the complete vocabulary distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZElDTWEHCHNO"
      },
      "source": [
        "### Evaluate!\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783,
          "referenced_widgets": [
            "ab36b61c6fce47b88977a84479b4b6d9",
            "e6b1036a9e4246f6ab5cd0edce48aef8",
            "c5f2a99bf6be4eef8947484547a53118",
            "d1f7e594f136476ab9b98bb86395722b",
            "cf62bd40d2a9445aac806f7cf4b997ba",
            "092743f17abd42ba9887662b4fa96285",
            "1e253050df8445c28bcaa05fc3db2633",
            "c4c217af35164e50b3152592e3e2bc86",
            "b674c4f110e24238ae9424b74734d63d",
            "bd5b50cd8fc24404b2e822f05046e292",
            "c81f35a29bab45b081d9f3e473a738c8",
            "61ae32f78f144cdb8581d134306b42b8",
            "c64a9af92b40471eb18ae45f1c8660e6",
            "6f23c4b4fd834d43a222d803b51bc719",
            "c3a5e6e0845e442a99d10e8521f6b905",
            "5aabad0fcbc74045985c9db02dcf3c6b",
            "56ecd62f62214ecbb284f3c8b46529c9",
            "8440a278c52d4874bb56c6a228ac32a8",
            "230378bb78e0488c98d6fdd935a73c88",
            "580387de511041f6847caf7043192bed",
            "e44384ad098a49c0ab3386d3045c0766",
            "4752610835344a798b4975451a8b7074",
            "43efe1f671e74494b6b9c876173fc45f",
            "d7516f4d4f7d49bfaf330d96350ec126",
            "17813ccac19a414bb8c3ed807253a059",
            "f8ba20e812d94f4f84ea7169737662f7",
            "b4b9f37e264449f694cff81c8774ea46",
            "91851cf8fd7044c18f8e8b7889916b88",
            "7f857442c42d4334a19de06f182ebaf2",
            "cff02e2bcbb040a0a8a6653a3f212bad",
            "f8574db2af064d6b820d556b48927b49",
            "2ac112600d30425d9bc2fd5cb35f7614",
            "54b230eaa0e04be1a0c55c3bbbb41ae7",
            "81afad5e109b42f5b838d958236a8f76",
            "60a9251c6b2446019f7e2fc9487ce7c7",
            "20f6edc453b54af2b9e4c341393a9512",
            "830f27cad4fb4f1f9bd8607ddc5a078b",
            "4d0b4e467f9642e69ac3bba66fc8d1b6",
            "dafa9b38bcb240fd9727da32243e1e4e",
            "72208c0915fc49e49c8b22d3e376257b",
            "dceec367c5364bf5a4e6df0ab4fe5144",
            "34f54a45b228488db83f058159dfa02e",
            "090d6aaad1a44ef9ae99d1e840ad519e",
            "afe192ec63b44e57be5a6bc9b8e7a4c0",
            "2050e05bd8c34b8299706cd8f4018f57",
            "7a046a8182a5499fb12e8504f5f09836",
            "d7a67e78fcd64c62b9efd93684781430",
            "60bdc33df3f049d9a13450f5abf69fe7",
            "ed8eff95baa3422886ac9554bd15d75f",
            "de3fd8577d1c4bce9210e6575774daa6",
            "6d986f6c1d6f45dab204011d54f47485",
            "be28103e5ac5435982b59d3efc6817e9",
            "07796e14b3b74608aeee39044d06d193",
            "ffc6d0109e8940e388efe99772820a21",
            "bdde1e75ee7c47a2918cbd5fa2e615ef",
            "2a7e5c0378814a0fbecfe74b32b88179",
            "b37ea299c9844d80b53e24a8c3943e49",
            "25b99bb38d7841aca9fa781954d44628",
            "d5d91130f94d45dba4b517062244610d",
            "e75b343bd7da4782a66beb92cf0bafcd",
            "17e57ab1dada45a0bb653e88c47ecb61",
            "9184514019b6493a877224685d03c2ac",
            "1845b4cba4d44fdbaf06f6ed20cfa29f",
            "90409078ef6e4352bb414c16ebe66f24",
            "9c49cc3738594812891213a1a71b3270",
            "f8a3a543a6fa4fde803a8a9951b8831a",
            "178a4edeaffe4db49af253c49e6e26fb",
            "679735131d6b4cf1a5775ba3ee5aad1d",
            "ba2a4a67d9904e0cb3d876662f0b4c4e",
            "dc781b852f384920965bc357af9cee8c",
            "05d869fdc80945dbabd81bc7916df2c7",
            "d99c201835274893a3a5fcb864a5dfb5",
            "989c312b08dc45459734e2817df03828",
            "49b915b8f2074e2f92875183524671ee",
            "0664e0fe538c49beb249dee9f1250389",
            "446c1f64c8284fd28aa6de4700168a23",
            "24655c91e23e4bfa81505ae27a559231",
            "aaa020b9dfb8436ca7837e5c2ecb4b49",
            "85ba11ae1d1a48ea9f0d5d7772f3d4c3",
            "82fcfb9ba735465189c35ee7ae51429b",
            "a6ad8af5fb0f41bdb0040a186fc6cf59",
            "a4ab422111c84a2f892a55b690fc3bcc",
            "66f6a108c10d489fac6668be9353ad08",
            "e03860a2a39b4479afc931ca72219331",
            "d72351616ea14f8e961c4f5313b48193",
            "4dc99d96c3994838871f0d43a56448ff",
            "b98c773e07e242778531deb7daf84811",
            "233d1d03b82e43a4aab310af8a54a645",
            "4835cede84b14a8a881256031673c0b7",
            "52e61a4b451d499b9a2e74288a29c20e",
            "a6e57b1a6ac8459aa5cf098657ca9ec5",
            "ee844893e50142a3af5406443aa86390",
            "3b5fedff9020497a84c57894d4848a00",
            "6f7ffcd86c534e949ac7b6679fb13782",
            "9011dfa7f4a14b00b653d7cfa40b13ac",
            "6a7192b7b5f44f4b9bb0443e63668d6e",
            "49391832826b4c7eab6470912675cce7",
            "0e6c027f7de941e3b4e0a11433165f47",
            "dbf5582f8d9941a68a8062fcea620e8e",
            "7cd1c78d0e164bfcbae460e5c4e16b9a",
            "884f304bc0544c99ba6e5fb1ee09713d",
            "3cd0f2fdaee8493999e11c8038878639",
            "779cf703701b47d2bf6fd5af6df4d198",
            "2d087bffc4e9483291e1fadaed2dc119",
            "6fd74de3deda44789cf899dc34276b05",
            "ae0b04fef7554c719d61bffeff1ffbae",
            "24332068f99a4299b53d6f4c2cab53ab",
            "033ce9ebfaea49119fd4b70157031b67",
            "a41e623d8db64841bd0cee3a27753e3e",
            "e49c902264044e95a84fa02d410f142b"
          ]
        },
        "id": "2UjZ-31s449u",
        "outputId": "d809fc5a-38b5-4c32-b051-b2f4c40bac89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab36b61c6fce47b88977a84479b4b6d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61ae32f78f144cdb8581d134306b42b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "greedy\n",
            "perplexity = 2.08\n",
            "fluency = 0.78\n",
            "diversity = 0.01, 0.02, 0.03\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43efe1f671e74494b6b9c876173fc45f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81afad5e109b42f5b838d958236a8f76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample\n",
            "perplexity = 70.58\n",
            "fluency = 0.34\n",
            "diversity = 0.43, 0.90, 0.99\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2050e05bd8c34b8299706cd8f4018f57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a7e5c0378814a0fbecfe74b32b88179",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temperature\n",
            "perplexity = 16.61\n",
            "fluency = 0.68\n",
            "diversity = 0.33, 0.79, 0.95\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "178a4edeaffe4db49af253c49e6e26fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaa020b9dfb8436ca7837e5c2ecb4b49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topk\n",
            "perplexity = 12.24\n",
            "fluency = 0.74\n",
            "diversity = 0.26, 0.75, 0.96\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4835cede84b14a8a881256031673c0b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cd1c78d0e164bfcbae460e5c4e16b9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topp\n",
            "perplexity = 12.33\n",
            "fluency = 0.72\n",
            "diversity = 0.29, 0.76, 0.96\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [item['prompt'] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in ['greedy', 'sample', 'temperature', 'topk', 'topp']:\n",
        "  generations = []\n",
        "  for prompt in tqdm(prompts):\n",
        "    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n",
        "  evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjlTLFH94Qlt"
      },
      "source": [
        "**TODO**: Which method has the best and worst perplexity? Fluency? Diversity?\n",
        "\n",
        "Best perplexity has the greedy (2.08) lowest perplexity,but it most predictable.The worst perplexity  is the sample (70.58)which has the highest perplexity and least predictable.The best fluency is the greedy (0.78) with the highest perplexity,least predictable.The best diversity is the sample (0.43,0.90,0.99) whiich has the highest diversity scores.The worst diversity is the greedy (0.01,0.02,0.03) where it has no diversity and also not  repetitive .\n",
        "\n",
        "Greedy has the excellent perplexity and fluency but terrible diversity where the sample has exccellent diversity where it has poor perplexity and fluency .Temperature,top k,top p offer balanced middle grounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv0P_mt1RWh6"
      },
      "source": [
        "## Beam Search\n",
        "\n",
        "In this part of the assignment, your main task is to implement beam search algorithm. While debugging for implementation, we recommend setting `device` to `cpu`, as debugging would not require too much resource for this assignment. Once you are done with implementation, you may switch `device` to `cuda` and choose `Colab Runtime Type = GPU` for faster evaluation of your algorithm.\n",
        "\n",
        "We have provided some helpful functions and classes. Please make sure that you understand them. But you don't need to implement/change anything of them until the code block marked TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPbvq4LnRd_j"
      },
      "source": [
        "### Configurations: load model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78dmI0HW4P-h"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq9lOjrwRSrs"
      },
      "source": [
        "### Helper classes: `BeamHypothesisList` and `BeamManager`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut9kXr0LRf4n"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BeamHypothesis:\n",
        "    def __init__(self, input_ids: torch.LongTensor, score: torch.FloatTensor | float):\n",
        "        self.input_ids: torch.LongTensor = input_ids  # a single token sequence of size (seq_len,)\n",
        "        self.score: torch.FloatTensor = score  # a scalar score for the token sequence\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"BeamHypothesis(input_ids: {self.input_ids}, score: {self.score})\"\n",
        "\n",
        "class BeamHypothesisList:\n",
        "    def __init__(self, beam_width: int):\n",
        "        self.beam_hypotheses: List[BeamHypothesis] = []  # list of beam_hypothesis\n",
        "        self.beam_width: int = beam_width\n",
        "\n",
        "        self.worst_score = 1e9  # worst beam score in self.beam_hypotheses\n",
        "\n",
        "    def add(self, new_input_ids: torch.LongTensor, sum_logprobs: float):\n",
        "        \"\"\"\n",
        "        :param new_input_ids: new token sequence of size (1, seq_len)\n",
        "        :param sum_logprobs: sum of log probabilities of tokens in new_input_ids\n",
        "        Given a new hypothesis (new_input_ids) and its corresponding sum_logprobs, update self.beam_hypotheses with a finished hypothesis.\n",
        "        (1) If the new_input_ids has higher score than the current worst score in self.beam_hypotheses,\n",
        "            we replace the worst one with the new hypothesis.\n",
        "        (2) Otherwise, we do not make any change to self.beam_hypotheses.\n",
        "        \"\"\"\n",
        "        # For score, we compute average token log probability\n",
        "        score = sum_logprobs / new_input_ids.size(-1)\n",
        "\n",
        "        # add the new hypothesis if we still have vacant beams\n",
        "        if len(self.beam_hypotheses) < self.beam_width:\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis: BeamHypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # even if the beam_hypotheses are full, if the new hypothesis has higher score than the worst hypothesis, we replace the worst hypothesis\n",
        "        elif score > self.worst_score:\n",
        "            # Remove the worst hypothesis, the one with the lowest score in self.beam_hypotheses\n",
        "            worst_hypothesis: BeamHypothesis = min(self.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "            self.beam_hypotheses.remove(worst_hypothesis)\n",
        "\n",
        "            # Add new hypothesis\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # Update the worst score - the lowest score among all beams in self.beam_hypotheses\n",
        "        self.worst_score = min(float(hyp.score) for hyp in self.beam_hypotheses)\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(self.beam_hypotheses) <= self.beam_width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HD5CoYBSvpE"
      },
      "outputs": [],
      "source": [
        "class BeamManager:\n",
        "    def __init__(self, batch_size: int, beam_width: int):\n",
        "        self.finished_beam_hypotheses_list = [BeamHypothesisList(beam_width) for _ in range(batch_size)]\n",
        "        self.batch_size = batch_size\n",
        "        self.beam_width = beam_width\n",
        "\n",
        "    def process(self,\n",
        "                input_ids: torch.LongTensor,\n",
        "                top_token_scores: torch.FloatTensor,\n",
        "                top_token_indices: torch.LongTensor,\n",
        "                top_token_beam_indices: torch.LongTensor\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_size * beam_width, current_seq_length), the input_ids that were used to compute top_tokens\n",
        "        :param top_token_scores: (batch_size, 2 * beam_width), representing the score of each top token\n",
        "        :param top_token_indices: (batch_size, 2 * beam_width), representing each token's index (in vocabulary) of the top tokens\n",
        "        :param top_token_beam_indices: (batch_size, 2 * beam_width), representing each token's corresponding beam index of the top tokens\n",
        "\n",
        "        Note: the input arguments `top_token_*` for each sample in batch are sorted from the largest score to the smallest score.\n",
        "        For example, if batch_size = 2 and beam_width = 3, then each of these values denote\n",
        "        top_token_indices[1, 2]: what is the third-best next token for the second sample in the batch?\n",
        "        top_token_scores[0, 1]: what is the score of the second-best next token for the first sample in the batch?\n",
        "        top_token_beam_indices[0, 1]: which beam did we use to generate the second-best next token for the first sample in the batch?\n",
        "\n",
        "        In this function, for each of the top-(2 * beam_width) tokens, we do the following:\n",
        "        (1) If the top token is EOS token:\n",
        "            This means that this hypothesis is done. Therefore, we save the hypothesis so-far to self.finished_beam_hypotheses_list.\n",
        "        (2) If the top token is not EOS token\n",
        "            We have to keep searching with this hypothesis. Therefore, we prepare the hypothesis for next time step.\n",
        "\n",
        "        Returns a dictionary, where\n",
        "        \"unfinished_scores\": size (batch_size * beam_width,), the score of the unfinished beams\n",
        "        \"unfinished_token_indices\": size (batch_size * beam_width,), the index of the last token in the unfinished beams\n",
        "        \"unfinished_beam_indices\": the index of the beam that was used to generate the new unfinished beam\n",
        "        \"\"\"\n",
        "        device = top_token_scores.device\n",
        "\n",
        "        # Initialize unfinished_token_*, which we will return for the next time step.\n",
        "        unfinished_scores = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_scores.dtype).to(device)  # score of the unfinished beams\n",
        "        unfinished_token_indices = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_indices.dtype).to(\n",
        "            device)  # index of the last token of the unfinished beams\n",
        "        unfinished_token_beam_indices = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_beam_indices.dtype).to(\n",
        "            device)  # index of the unfinished beam in the batch\n",
        "\n",
        "        # Loop over the batch\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get the top_token_scores, top_token_indices, top_token_beam_indices for this sample in the batch\n",
        "            # NOTE: size of sample_top_token_*: (2 * beam_width,)\n",
        "            sample_top_token_scores = top_token_scores[batch_idx]\n",
        "            sample_top_token_indices = top_token_indices[batch_idx]\n",
        "            sample_top_token_beam_indices = top_token_beam_indices[batch_idx]\n",
        "\n",
        "            # Loop over all top tokens\n",
        "            sample_beam_idx = 0\n",
        "            for top_token_score, top_token_index, top_token_beam_index in zip(\n",
        "                    sample_top_token_scores, sample_top_token_indices, sample_top_token_beam_indices\n",
        "            ):\n",
        "                # Note that top_token_beam_indices only denotes the index of the beam in each sample.\n",
        "                # We transform this into `beam_idx_in_batch`, where we denote the index of the beam among all (batch_size * beam_width) beams in the batch.\n",
        "                beam_idx_in_batch = batch_idx * self.beam_width + top_token_beam_index\n",
        "\n",
        "                # if top_token == EOS, we add the generation so-far to the beam_hypotheses_list\n",
        "                if top_token_index.item() == eos_token_id:\n",
        "                    # Among the (batch_size * beam_width) input_ids, find the input_ids that correspond to this top_token\n",
        "                    # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                    new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                    # Add the new beam to sample_beam_hypothesis_list\n",
        "                    sample_beam_hypothesis_list.add(\n",
        "                        new_input_ids,\n",
        "                        top_token_score\n",
        "                    )\n",
        "\n",
        "                # if top_token =/= EOS, we aggregate them for next time step.\n",
        "                else:\n",
        "                    # Store the score, token_index, beam_idx_in_batch to the unfinished_scores, unfinished_token_indices, unfinished_token_beam_indices\n",
        "                    unfinished_scores[batch_idx, sample_beam_idx] = top_token_score\n",
        "                    unfinished_token_indices[batch_idx, sample_beam_idx] = top_token_index\n",
        "                    unfinished_token_beam_indices[batch_idx, sample_beam_idx] = beam_idx_in_batch\n",
        "\n",
        "                    sample_beam_idx += 1\n",
        "\n",
        "                # once we have `beam_width` number of new beams, we don't have to add anymore.\n",
        "                if sample_beam_idx == self.beam_width:\n",
        "                    break\n",
        "\n",
        "        # Return the dictionary of unfinished_scores, unfinished_token_indices, unfinished_beam_indices\n",
        "        # Make sure to change the size of each tensor to (batch_size * beam_width,)\n",
        "        return {\n",
        "            \"unfinished_scores\": unfinished_scores.view(-1),  # (batch_size * beam_width,)\n",
        "            \"unfinished_token_indices\": unfinished_token_indices.view(-1),  # (batch_size * beam_width,)\n",
        "            \"unfinished_beam_indices\": unfinished_token_beam_indices.view(-1),  # (batch_size * beam_width,)\n",
        "        }\n",
        "\n",
        "    def finalize(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            beam_scores: torch.FloatTensor,\n",
        "    ) -> Tuple[List[torch.LongTensor], List[torch.FloatTensor]]:\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_idx * beam_width, max_length), input_ids of unfinished beams\n",
        "        :param beam_scores: (batch_idx * beam_width,), scores of unfinished beams\n",
        "        Get the final best beams, among\n",
        "        (1) unfinished beams, for which we get the input_ids and beam_scores as arguments\n",
        "        (2) finished beams, which we store in self.batch_beam_hypothesis_list\n",
        "        Returns a tuple of two lists, where\n",
        "        - tuple[0] is the list of the input_ids of the best beams (length: batch_idx)\n",
        "        - tuple[1] is the list of the scores of the best beams (length: batch_idx\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Add all unfinished beam hypotheses to self.finished_beam_hypotheses_list\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            for sample_beam_idx in range(self.beam_width):\n",
        "                # Get beam_idx_in_batch: index of the beam in all `batch_size * beam_width` beams in the batch\n",
        "                beam_idx_in_batch = batch_idx * self.beam_width + sample_beam_idx\n",
        "\n",
        "                # Get the input_id for this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                # Get the score of this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: beam_score should be a scalar\n",
        "                beam_score = beam_scores[beam_idx_in_batch].item()\n",
        "\n",
        "                # Add the new hypothesis to sample_beam_hypothesis_list\n",
        "                sample_beam_hypothesis_list.add(new_input_ids, beam_score)\n",
        "\n",
        "        # 2. Select the best hypothesis from each beam_hypothesis_list\n",
        "        best_input_ids = []\n",
        "        best_scores = []\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get best_hypothesis among sample_beam_hypothesis_list (the one with the highest score)\n",
        "            best_hypothesis = max(sample_beam_hypothesis_list.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "\n",
        "            # Save the input_ids and score of best_hypothesis\n",
        "            best_input_ids.append(best_hypothesis.input_ids)\n",
        "            best_scores.append(best_hypothesis.score)\n",
        "\n",
        "        return best_input_ids, best_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXle5lWRRVma"
      },
      "source": [
        "### Beam Search\n",
        "\n",
        "Conceptually, beam search is a straightforward extension to greedy decodingâ€”where we retain the top-k hypotheses instead of the top-1 hypothesis in greedy decoding. However, implementing it is much more complex than the intuition. While there are many â€œarbitraryâ€ decisions one needs to make to implement beam search, for the purpose of this assignment, we will do the following:\n",
        "\n",
        "* Compared to greedy decoding, beam search introduces one additional hyper-parameter, beam_width, denoting the number of hypotheses we retain at each time step. For example, set beam_width = 3.\n",
        "\n",
        "* At each time step, we have access to 3 hypotheses $h_1, h_2, h_3$ generated so far, along with the corresponding scores $s_1, s_2, s_3$ (typically defined as the average log probability of tokens in each hypothesis). To update the hypothesis, we compute next token distribution $p_i$ for each hypothesis $h_i$, then take top-`beam_width` tokens (that makes the highest score when appended to the corresponding hypothesis) across all $p_i$s.\n",
        "\n",
        "* At each time step, in some of the hypotheses (say $h_1$), we may encounter the EOS token `<EOS>`. In greedy decoding, we can simply terminate the generation process, but in beam search, there still remain the 2 unfinished hypotheses. In this case, we move the finished hypothesis ($h_1$, which encountered the EOS token) into a separate list. For the remaining unfinished hypotheses $h_2$ and $h_3$, we keep searching for the best next tokens, retrieving the top `beam_width` = 3 tokens to create 3 new hypotheses (and their corresponding scores) for the next time step.\n",
        "\n",
        "* After reaching the max length step, we have two sets of hypothesesâ€”a set of unfinished hypotheses that did not encounter `<EOS>` until the final step, and a set of finished hypotheses. Finally, we compare all hypotheses across the two sets, and return the one with the highest score. Fill in the TODOs in `beam search()`. Given the complexity of the implementation, we provide more\n",
        "structured skeleton for the algorithm, along with two helper classes `BeamHypothesisList` and `BeamManager`. Note that the two classes are already fully implemented, your job is to finish `beam search()` using the\n",
        "two helper classes. You may find the [beam search implementation in Huggingface transformers](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py##L2743) useful. In\n",
        "addition, we leave comments for the expected size of tensor variables as a sanity check.\n",
        "\n",
        "**TODO: fill in the TODOs in `beam_search`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JTN6KpYS0P-"
      },
      "outputs": [],
      "source": [
        "# TODO: Complete the implementation of beam search.\n",
        "\n",
        "def beam_search(prompts: List[str], beam_width: int, max_length: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param prompts: list of prompt strings\n",
        "    :param beam_width: number of hypotheses in the beam\n",
        "    :param max_length: max generation length\n",
        "    :return: list of generation, including both the original prompt and generation\n",
        "    \"\"\"\n",
        "    # TODO: encode the prompts using tokenizer, to get input_ids and attention_mask\n",
        "    input_encoding = tokenizer(prompts, return_tensors=\"pt\", padding='longest')\n",
        "    input_ids = input_encoding['input_ids'].to(device)\n",
        "    attention_mask = input_encoding['attention_mask'].to(device)\n",
        "\n",
        "    if input_ids.size(-1) > max_length:\n",
        "      raise ValueError(\"Input ID is larger than max_length.\")\n",
        "\n",
        "    # --- Do not change below --- #\n",
        "    batch_size = input_ids.size(0)\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # initialize model_kwargs\n",
        "    model_kwargs = {\n",
        "        'attention_mask': attention_mask,\n",
        "        'cache_position': torch.arange(input_ids.shape[1], device=device)\n",
        "    }\n",
        "\n",
        "    # interleave input_ids according to beam_width.\n",
        "    # For example, input_ids for [\"Hi\", \"good\"] with beam_width=3 becomes [\"Hi\", \"Hi\", \"Hi\", \"good\", \"good\", \"good\"]\n",
        "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
        "        input_ids=input_ids,\n",
        "        expand_size=beam_width,\n",
        "        is_encoder_decoder=False,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "    # NOTE: the type of `input_ids` and `model_kwargs` are as following:\n",
        "    # input_ids: tensor of size (batch_size * beam_width, seq_len)\n",
        "    # model_kwargs: a dictionary with two elements 'attention_mask', sized (batch_size * beam_width, seq_len), and 'cache_position'\n",
        "    # --- Do not change above --- #\n",
        "\n",
        "    # TODO: initialize beam_manager\n",
        "    beam_manager = BeamManager(batch_size,beam_width)\n",
        "\n",
        "    # TODO: initialize unfinished_beam_scores, a tensor of size (batch_size, beam_width) with all elements = 0\n",
        "    unfinished_beam_scores = torch.zeros((batch_size,beam_width),dtype=torch.float,device = device)\n",
        "\n",
        "    # For each sample in the batch, set all initial beam_score to -1e9, except for the first beam\n",
        "    unfinished_beam_scores[:, 1:] = -1e9\n",
        "    unfinished_beam_scores = unfinished_beam_scores.view(-1)  # (batch_size * beam_width,)\n",
        "\n",
        "    while True:\n",
        "        # --- Do not change below --- #\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # TODO: run model forward pass with model_inputs as the input\n",
        "        # NOTE: we should set return_dict=True, output_attentions=False and output_hidden_states=False\n",
        "        model_outputs = model(**model_inputs,return_dict = True,output_attentions=False,output_hidden_states=False)\n",
        "\n",
        "\n",
        "        # TODO compute log_probs for next tokens, using `model_outputs.logits`\n",
        "        # NOTE: size of next_token_scores: (batch_size * beam_width, vocab_size)\n",
        "        next_token_logits = model_outputs.logits[:,-1,:]\n",
        "\n",
        "        next_token_scores = F.log_softmax(next_token_logits,dim = -1)\n",
        "\n",
        "        # TODO add previous beam_scores to the next_token_scores\n",
        "        # NOTE: size of new_scores: # (batch_size * beam_width, vocab_size)\n",
        "        new_scores = next_token_scores + unfinished_beam_scores.unsqueeze(-1)\n",
        "\n",
        "        # TODO: retrieve top-(2 * beam_width) next tokens for each sample in the batch\n",
        "        # NOTE: size of `top_token_scores` and `top_token_indices` needs to be: (batch_size, 2 * beam_width)\n",
        "        new_scores = new_scores.view(batch_size, beam_width * vocab_size)\n",
        "        # NOTE: `top_token_scores` and `top_token_indices` should be sorted from the one with larget score to the one with smallest score (for each sample in batch)\n",
        "        # Hint: use torch.topk with largest=True, sorted=True\n",
        "        # Hint: new_scores needs to be transformed to shape (batch_size, beam_width * vocab_size) prior to topk operation.\n",
        "        top_token_scores, top_token_indices = torch.topk(new_scores,2*beam_width,dim = -1,largest=True,sorted = True)\n",
        "\n",
        "        # Since top_token_indices are over beam_width * vocab_size, divide it by beam_width to get vocabulary index and beam index\n",
        "        top_token_beam_indices = torch.div(top_token_indices, vocab_size,\n",
        "                                           rounding_mode=\"floor\")  # from which beam the top-token was retrieved from\n",
        "        top_token_indices = top_token_indices % vocab_size  # the index of top-token in the vocabulary\n",
        "\n",
        "        # TODO: Run beam_manager.process and save the results in unfinished_beam_scores, unfinished_token_indices and unfinished_beam_indices\n",
        "        unfinished_beam_outputs = beam_manager.process(input_ids,top_token_scores,top_token_indices,top_token_beam_indices)\n",
        "        unfinished_beam_scores = unfinished_beam_outputs[\"unfinished_scores\"]\n",
        "        unfinished_token_indices = unfinished_beam_outputs[\"unfinished_token_indices\"]\n",
        "        unfinished_beam_indices = unfinished_beam_outputs[\"unfinished_beam_indices\"]\n",
        "\n",
        "        # --- Prepare input_ids for next time step --- #\n",
        "        # TODO: index input_ids with the unfinished beam indices\n",
        "        # NOTE: input_ids should be (batch_size * beam_width, seq_len)\n",
        "        input_ids = input_ids[unfinished_beam_indices]\n",
        "\n",
        "        # TODO: concatenate the unfinished_token_indices to the input_ids\n",
        "        input_ids = torch.cat([input_ids, unfinished_token_indices.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # --- Do not change below --- #\n",
        "        # update the model_kwargs according to the concatenated input_ids\n",
        "        model_kwargs = model._update_model_kwargs_for_generation(\n",
        "            model_outputs, model_kwargs, is_encoder_decoder=False\n",
        "        )\n",
        "        if model_kwargs[\"past_key_values\"] is not None:\n",
        "            model_kwargs[\"past_key_values\"] = model._temporary_reorder_cache(\n",
        "                model_kwargs[\"past_key_values\"], unfinished_beam_indices,\n",
        "            )\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # if unfinished input_ids reach the max seq length, exit the loop\n",
        "        if input_ids.size(-1) == max_length:\n",
        "            break\n",
        "\n",
        "    # TODO: Run beam_manager.finalize to get the best_input_ids and best_scores, among all finished / unfinished beams\n",
        "    best_input_ids, best_scores = beam_manager.finalize(input_ids,unfinished_beam_scores)\n",
        "\n",
        "    # if len(best_input_ids) < max_length, pad them to the max length\n",
        "    for batch_idx, sample_input_ids in enumerate(best_input_ids):\n",
        "        if sample_input_ids.size(-1) < max_length:\n",
        "            pad_tensor = torch.LongTensor([pad_token_id] * (max_length - sample_input_ids.size(-1))).to(device)\n",
        "\n",
        "            # TODO: pad best_input_ids with pad_tensor\n",
        "            best_input_ids[batch_idx] = torch.cat([sample_input_ids,pad_tensor])\n",
        "\n",
        "    # TODO: transform best_input_ids (which is currently a list of tensors) into a tensor of size (batch_idx, max_seq_length)\n",
        "    # Hint: use torch.stack\n",
        "    best_input_ids = torch.stack(best_input_ids)\n",
        "\n",
        "    return tokenizer.batch_decode(best_input_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Az-Q7QlywzZ"
      },
      "source": [
        "### Sanity check for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuvQmmYWS3FX",
        "outputId": "3fa7fb40-8765-4f16-c09a-077760396755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"This restaurant is awesome. I've been here a few\",\n",
              " 'My dog is cute and I love it.\\n\\nRated 5 out of',\n",
              " 'Today is sunny. The sun is shining. The']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents = [\n",
        "    \"This restaurant is awesome.\",\n",
        "    \"My dog is cute and I love it.\",\n",
        "    \"Today is sunny.\",\n",
        "]\n",
        "\n",
        "beam_search(sents, beam_width=5, max_length=15)\n",
        "\n",
        "# expected output: [\"This restaurant is awesome. I've been here a few\", 'My dog is cute and I love it.\\n\\nRated 5 out of', 'Today is sunny. The sun is shining. The']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYkg7i5ayyy_"
      },
      "source": [
        "### Evaluate Beam Search!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "5143892476544bb79fa8bfad781fd9b3",
            "2ee69f8a7aae4bd2982447f29a2e9ee9",
            "c798b9e09f624dd691b4ad7bff2d1d1e",
            "f4b3a0859ead47d599230f70916a5451",
            "34bfb0cf1d8145b7a98ac5611d6f161f",
            "2952c28dac4a49b394094d307b541e1f",
            "8cd4dd3e2332468da7197eb1a5ca6cf6",
            "4f475f356ab74573b1984a9866be4b39",
            "e1d73a320adc470c9f353933bee68b74",
            "a746f467b99d4077a089054bc968255b",
            "781bb98267c64203beca17ec52b3a5ee",
            "54a0532f9c1548f69124e84bf623d7c5",
            "2d7582a86ca34f3b8b76f3135045b73a",
            "990e46a3a96f446aa7e0a613f33e385f",
            "04d5203a32094cafa7daf58d71111860",
            "0504546ff5f64ac4a2f6a23e725c1e70",
            "ea134b13bacd4f8cb93500ae886460dc",
            "f718c6736665484083960f8b6be53dae",
            "6e88dff4313543b4916a265a80e0434f",
            "0961d8a15de94c1fa9762bae00c231be",
            "19859542d7c0479497a0a6643827c9d7",
            "6018ef7dd32b4d4d8ca011f9338dfee7"
          ]
        },
        "id": "HGSf-kEUS--e",
        "outputId": "59f2764a-f5e8-49f6-cf8a-0ca403e55487"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5143892476544bb79fa8bfad781fd9b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4107028903.py:51: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  self.worst_score = min(float(hyp.score) for hyp in self.beam_hypotheses)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54a0532f9c1548f69124e84bf623d7c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam Search\n",
            "perplexity = 2.05\n",
            "fluency = 0.82\n",
            "diversity = 0.07, 0.13, 0.17\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If your implementation is efficient enough, the following code will run in no longer than 3 minutes with device = gpu.\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "prompts = [item['prompt'] for item in test_data][:100]\n",
        "MAX_LEN = 100\n",
        "beam_width = 5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "generations = []\n",
        "for batch_start_idx in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
        "  batched_prompts = prompts[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
        "  batched_generations = beam_search(batched_prompts, beam_width, MAX_LEN)\n",
        "\n",
        "  # remove prompt from generation\n",
        "  batched_generations = [generation[len(prompt):] for prompt, generation in zip(batched_prompts, batched_generations)]\n",
        "\n",
        "  generations += batched_generations\n",
        "\n",
        "evaluate(generations, 'Beam Search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27zicApAtc7Y",
        "outputId": "2ba549ee-e104-4006-9c91-3fb1dca0d96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0\n",
            "The soccer game was tied 3 to 3 and there was a minute left to play.\n",
            "Generation 0\n",
            "\n",
            "\n",
            "\"I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not\n",
            "---------\n",
            "\n",
            "Prompt 1\n",
            "Molly loves popcorn.\n",
            "Generation 1\n",
            "\n",
            "\n",
            "\"I love popcorn,\" she said. \"I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn.\n",
            "---------\n",
            "\n",
            "Prompt 2\n",
            "Tim rented a car to visit his ill mother.\n",
            "Generation 2\n",
            "\n",
            "\n",
            "\"It was a very nice day,\" he said.\n",
            "\n",
            "\"It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very\n",
            "---------\n",
            "\n",
            "Prompt 3\n",
            "Max had been dating Maddie for three Year's.\n",
            "Generation 3\n",
            "\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating\n",
            "---------\n",
            "\n",
            "Prompt 4\n",
            "I'm waiting for a payment to come through the mail.\n",
            "Generation 4\n",
            " I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail\n",
            "---------\n",
            "\n",
            "Prompt 5\n",
            "Kennan was born with a proprioceptive disorder.\n",
            "Generation 5\n",
            "\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's\n",
            "---------\n",
            "\n",
            "Prompt 6\n",
            "Rich had worked in the woods for Years.\n",
            "Generation 6\n",
            "\n",
            "\n",
            "\"I'm not going to lie to you,\" he said. \"I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going\n",
            "---------\n",
            "\n",
            "Prompt 7\n",
            "Tanya wanted to see her sister.\n",
            "Generation 7\n",
            "\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"\n",
            "---------\n",
            "\n",
            "Prompt 8\n",
            "One afternoon Sallie took her dog, Gibby, to the dog park.\n",
            "Generation 8\n",
            "\n",
            "\n",
            "\"He was so happy,\" Sallie said. \"He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy\n",
            "---------\n",
            "\n",
            "Prompt 9\n",
            "Omar was walking his dog in a new neighborhood.\n",
            "Generation 9\n",
            "\n",
            "\n",
            "\"I was like, 'What the hell is going on?'\" he said. \"I was like, 'What the hell is going on?'\n",
            "\n",
            "\"I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I\n",
            "---------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print first 10 generations\n",
        "\n",
        "sampled_prompts = prompts[:10]\n",
        "sampled_generations = generations[:10]\n",
        "\n",
        "for idx, (prompt, generation) in enumerate(zip(sampled_prompts, sampled_generations)):\n",
        "  print(f\"Prompt {idx}\")\n",
        "  print(prompt)\n",
        "  print(f\"Generation {idx}\")\n",
        "  print(generation)\n",
        "  print(\"---------\")\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}